{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import contextlib\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21ad55784504541b92704045ec284d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = \"bigcode/starcoder2-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    quantization_config=None,\n",
    "    device_map=None,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# model = model.merge_and_unload()\n",
    "if not hasattr(model, \"hf_device_map\"):\n",
    "    model.cuda()\n",
    "\n",
    "# model_id = \"smangrul/peft-lora-starcoder15B-v2-personal-copilot-A100-40GB-colab\"\n",
    "model_id = \"data/peft-lora-starcoder2-7b-personal-copilot-dual-3090-local\"\n",
    "model = PeftModel.from_pretrained(model, model_id, adapter_name=\"copilot\")\n",
    "\n",
    "# model_id = \"smangrul/peft-lora-starcoder15B-v2-personal-copilot-A100-40GB-colab\"\n",
    "# model = PeftModel.from_pretrained(model, model_id, adapter_name=\"personal_copilot\")\n",
    "# model.add_weighted_adapter([\"personal_copilot\"], [0.8], \"best_personal_copilot\")\n",
    "# model.set_adapter(\"best_personal_copilot\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_code_completion(prefix, suffix):\n",
    "    text = prompt = f\"\"\"<fim_prefix>{prefix}<fim_suffix>{suffix}<fim_middle>\"\"\"\n",
    "    model.eval()\n",
    "    outputs = model.generate(\n",
    "        input_ids=tokenizer(text, return_tensors=\"pt\").input_ids.cuda(),\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.2,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.0,\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fim_prefix>from accelerate import Accelerator\n",
      "\n",
      "accelerator = Accelerator()\n",
      "\n",
      "model, optimizer, training_dataloader, scheduler = <fim_suffix><fim_middle>accelerator.prepare(model, optimizer, training_dataloader, scheduler)\n",
      "\n",
      "for epoch in range(num_epochs):\n",
      "    for batch in training_dataloader:\n",
      "        outputs = model(**batch)\n",
      "        loss = outputs.loss\n",
      "        accelerator.backward(loss)\n",
      "        optimizer.step()\n",
      "        optimizer.zero_grad()\n",
      "        scheduler.step()\n",
      "        accelerator.sync_gradients(model)\n",
      "```\n",
      "\n",
      "## Inference\n",
      "\n",
      "```python\n",
      "import torch\n",
      "\n",
      "model = torch.load(\"path/to/model.pt\")\n",
      "\n",
      "with torch.no_grad():\n",
      "    outputs = model(**batch)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\"from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, training_dataloader, scheduler = \"\"\"\n",
    "\n",
    "suffix = \"\"\"\"\"\"\n",
    "print(get_code_completion(prefix, suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fim_prefix>from peft import LoraConfig, TaskType, get_peft_model\n",
      "from transformers import AutoModelForCausalLM\n",
      "\n",
      "peft_config = LoraConfig(<fim_suffix>)<fim_middle>\n",
      "    r=64,\n",
      "    lora_alpha=32,\n",
      "    lora_dropout=0.1,\n",
      "    bias=\"none\",\n",
      "    task_type=TaskType.CAUSAL_LM,\n",
      ")\n",
      "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
      "model = get_peft_model(model, peft_config<|endoftext|># coding=utf-8\n",
      "# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\"\\\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "peft_config = LoraConfig(\"\"\"\n",
    "\n",
    "suffix = \")\"\n",
    "print(get_code_completion(prefix, suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fim_prefix>\n",
      "# Here is the correct implementation of the two sum code exercise\n",
      "# time complexity: O(N)\n",
      "# space complexity: O(N)\n",
      "def two_sum(arr, target_sum):\n",
      "<fim_suffix><fim_middle>    # create a hash table to store the values and their indices\n",
      "    hash_table = {}\n",
      "    for i, num in enumerate(arr):\n",
      "        # check if the complement is already in the hash table\n",
      "        if target_sum - num in hash_table:\n",
      "            return [hash_table[target_sum - num], i]\n",
      "        # store the value and its index in the hash table\n",
      "        hash_table[num] = i\n",
      "    return None\n",
      "<fim_middle>    # iterate over the array\n",
      "    for i, num in enumerate(arr):\n",
      "        # check if the complement is in the array\n",
      "        if target_sum - num in\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\"\n",
    "# Here is the correct implementation of the two sum code exercise\n",
    "# time complexity: O(N)\n",
    "# space complexity: O(N)\n",
    "def two_sum(arr, target_sum):\n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\"\"\"\n",
    "print(get_code_completion(prefix, suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fim_prefix>import math\n",
      "import re\n",
      "import warnings\n",
      "from dataclasses import asdict, dataclass, field, replace\n",
      "from enum import Enum\n",
      "from typing import List, Optional, Tuple, Union\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from tqdm import tqdm\n",
      "from transformers.pytorch_utils import Conv1D\n",
      "\n",
      "from..config import PeftConfig\n",
      "from..import_utils import is_bnb_4bit_available, is_bnb_available\n",
      "from..utils import (\n",
      "    CLAMP_QUANTILE,\n",
      "    COMMON_LAYERS_PATTERN,\n",
      "    TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING,\n",
      "    ModulesToSaveWrapper,\n",
      "    PeftType,\n",
      "    _freeze_adapter,\n",
      "    _get_submodules,\n",
      "    transpose,\n",
      ")\n",
      "from.tuners_utils import BaseTuner, BaseTunerLayer\n",
      "\n",
      "@dataclass\n",
      "class BottleneckAdapterConfig(PeftConfig):\n",
      "    \"\"\"\n",
      "    <fim_suffix>\n",
      "    \"\"\" <fim_middle>This is the configuration class to store the configuration of a [`BottleneckAdapter`]. It is used to instantiate a\n",
      "    BottleneckAdapter according to the specified arguments, defining the model architecture. Instantiating a\n",
      "    configuration with the defaults will yield a similar configuration to that of the\n",
      "    [microsoft/llama-2-7b-hf](https://huggingface.co/microsoft/llama-2-7b-hf) architecture.\n",
      "\n",
      "    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
      "    documentation from [`PretrainedConfig`] for more information.\n",
      "\n",
      "\n",
      "    Args\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\"import math\n",
    "import re\n",
    "import warnings\n",
    "from dataclasses import asdict, dataclass, field, replace\n",
    "from enum import Enum\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers.pytorch_utils import Conv1D\n",
    "\n",
    "from ..config import PeftConfig\n",
    "from ..import_utils import is_bnb_4bit_available, is_bnb_available\n",
    "from ..utils import (\n",
    "    CLAMP_QUANTILE,\n",
    "    COMMON_LAYERS_PATTERN,\n",
    "    TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING,\n",
    "    ModulesToSaveWrapper,\n",
    "    PeftType,\n",
    "    _freeze_adapter,\n",
    "    _get_submodules,\n",
    "    transpose,\n",
    ")\n",
    "from .tuners_utils import BaseTuner, BaseTunerLayer\n",
    "\n",
    "@dataclass\n",
    "class BottleneckAdapterConfig(PeftConfig):\n",
    "    \\\"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "    \\\"\"\" \\\n",
    "\"\"\"\n",
    "\n",
    "print(get_code_completion(prefix, suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".copilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
