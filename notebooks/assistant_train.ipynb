{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run assistant train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following configuration is tested working:\n",
    "* disable Flash_Attn\n",
    "* create peft_model with get_peft_model before calling SFTTrainer (without passing peft_config to the SFTrainer)\n",
    "\n",
    "If we pass `peft_config` to the `SFTTrainer` to create the `peft_model`, the following error is raised and need to be debugged:\n",
    "`RuntimeError: expected scalar type BFloat16 but found Float`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Environment (only need to do once!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create a virtual environment and install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "cat /etc/os-release\n",
    "nvcc -V\n",
    "cd personal_copilot\n",
    "python3.11 -m venv .copilot\n",
    "source .copilot/bin/activate\n",
    "pip install --upgrade pip setuptools wheel\n",
    "pip install torch torchvision torchaudio\n",
    "pip install packaging\n",
    "pip install flash-attn\n",
    "pip install -r training/requirements.txt\n",
    "pip install -r dateset_generation/requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "```shell\n",
    "python train.py \\\n",
    "    --model_name_or_path \"bigcode/starcoder2-7b\" \\\n",
    "    --lora_r 32 \\\n",
    "    --lora_alpha 64 \\\n",
    "    --lora_dropout 0.0 \\\n",
    "    --lora_target_modules \"c_proj,c_attn,q_attn,c_fc,c_proj\" \\\n",
    "    --use_nested_quant \\\n",
    "    --bnb_4bit_compute_dtype \"bfloat16\" \\\n",
    "    --use_flash_attn \\\n",
    "    --use_peft_lora \\\n",
    "    --use_4bit_quantization \\\n",
    "    --dataset_name \"smangrul/hug_stack\" \\\n",
    "    --dataset_text_field \"text\" \\\n",
    "    --max_seq_length 1024 \\\n",
    "    --fim_rate 0.5 \\\n",
    "    --fim_spm_rate 0.5 \\\n",
    "    --splits \"train\" \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --bf16 \\\n",
    "    --learning_rate 5e-4 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --max_steps 1000 \\\n",
    "    --warmup_steps 30 \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --evaluation_strategy \"steps\" \\\n",
    "    --eval_steps 50 \\\n",
    "    --save_steps 50 \\\n",
    "    --logging_steps 25 \\\n",
    "    --output_dir \"peft-lora-starcoder2-7b-personal-copilot-dual-3090-local\" \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "python train.py \\\n",
    "--model_name \"codellama/CodeLlama-7b-hf\" \\\n",
    "--lora_r 8 \\\n",
    "--lora_alpha 32 \\\n",
    "--lora_target_modules \"all-linear\" \\\n",
    "--use_nested_quant True \\\n",
    "--bnb_4bit_compute_dtype \"bfloat16\" \\\n",
    "--use_flash_attn True \\\n",
    "--use_peft_lora \\\n",
    "--use_4bit_quantization \\\n",
    "--dataset_name \"smangrul/code-chat-assistant-v1\" \\\n",
    "--dataset_text_field \"content\" \\\n",
    "--max_seq_len 512 \\\n",
    "--per_device_train_batch_size 1 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--bf16 True \\\n",
    "--learning_rate 5e-4 \\\n",
    "--lr_scheduler_type \"cosine\" \\\n",
    "--weight_decay 0.01 \\\n",
    "--save_steps 50 \\\n",
    "--dataloader_num_workers 4 \\\n",
    "--num_train_epochs 3 \\\n",
    "--logging_steps 25 \\\n",
    "--packing True \\\n",
    "--output_dir \"peft-lora-codellama-7b-chat-asst-dual-3090-local\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Tensorboard\n",
    "\n",
    "```shell\n",
    "cd personal_copilot/training/peft-lora-starcoder2-7b-personal-copilot-dual-3090-local\n",
    "tensorboard --logdir=runs --bind_all\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can run the training, let's go back to understand what is actually going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the parent directory to the path\n",
    "sys.path.append('../chat_assistant/sft/training')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = ['ipywidgets']  # Add your packages here\n",
    "\n",
    "for package in packages:\n",
    "    !pip show {package} > /dev/null || pip install {package}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "import sys\n",
    "from typing import Optional\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    set_seed,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser, \n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import DatasetDict, load_dataset, load_from_disk\n",
    "from datasets.builder import DatasetGenerationError\n",
    "from trl import SFTTrainer\n",
    "# from utils import (\n",
    "#     create_and_prepare_model,\n",
    "#     create_datasets,\n",
    "#     loftq_init,\n",
    "#     get_module_class_from_name,\n",
    "# )\n",
    "from train import ModelArguments, DataTrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with defining a `HfArgumentParser`: This module from the Hugging Face transformers library parses command-line arguments related to the model, data, and training configurations. \n",
    "\n",
    "* We can place all the arguments in a `json` file and use `parse_json_file`.\n",
    "* or place them in the command line and use `parse_args_into_dataclasses`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs \n",
    "\n",
    "#### from command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\n",
    "    \"--model_name\", \"bigcode/starcoder2-3b\",\n",
    "    \"--lora_r\", \"8\",\n",
    "    \"--lora_alpha\", \"32\",\n",
    "    \"--lora_target_modules\", \"all-linear\",\n",
    "    \"--use_nested_quant\", \"True\",\n",
    "    \"--bnb_4bit_compute_dtype\", \"bfloat16\",\n",
    "    \"--use_flash_attn\", \"False\",\n",
    "    \"--use_peft_lora\",\n",
    "    \"--use_4bit_quantization\",\n",
    "    \"--dataset_name\", \"smangrul/code-chat-assistant-v1\",\n",
    "    \"--dataset_text_field\", \"content\",\n",
    "    \"--max_seq_len\", \"512\",\n",
    "    \"--per_device_train_batch_size\", \"1\",\n",
    "    \"--gradient_accumulation_steps\", \"1\",\n",
    "    \"--bf16\",\n",
    "    \"--learning_rate\", \"5e-4\",\n",
    "    \"--lr_scheduler_type\", \"cosine\",\n",
    "    \"--weight_decay\", \"0.01\",\n",
    "    \"--save_steps\", \"50\",\n",
    "    \"--dataloader_num_workers\", \"4\",\n",
    "    \"--num_train_epochs\", \"3\",\n",
    "    \"--logging_steps\", \"25\",\n",
    "    \"--packing\", \"True\",\n",
    "    \"--output_dir\", \"peft-lora-starcoder2-3b-chat-asst-dual-3090-local\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse arguments\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_and_prepare_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = model_args\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = data_args\n",
    "data_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = training_args\n",
    "training_args;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_map = None\n",
    "bnb_config = None\n",
    "quant_storage_stype = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = model_args.use_8bit_qunatization\n",
    "load_in_8bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.use_unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_args.use_unsloth:\n",
    "    from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_4bit = model_args.use_4bit_quantization\n",
    "load_in_4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    torch.distributed.is_available()\n",
    "    and torch.distributed.is_initialized()\n",
    "    and torch.distributed.get_world_size() > 1\n",
    "    and args.use_unsloth\n",
    "):\n",
    "    raise NotImplementedError(\"Unsloth is not supported in distributed training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization & bnb config\n",
    "\n",
    "We are using [QLoRA](https://huggingface.co/papers/2305.14314). QLoRA is a method for fine-tuning models that employs a two-pronged approach. \n",
    "\n",
    "Firstly, it quantizes the model to 4-bits, thereby reducing the computational resources required. \n",
    "\n",
    "Secondly, it incorporates a set of Low-Rank Adaptation (LoRA) weights into the model, which are fine-tuned via the quantized weights. \n",
    "\n",
    "In addition to the conventional Float4 data type (LinearFP4), QLoRA introduces a new 4-bit NormalFloat (LinearNF4) data type. This new data type is specifically designed for quantizing normally distributed data, and can enhance the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_4bit_compute_dtype = model_args.bnb_4bit_compute_dtype\n",
    "bnb_4bit_compute_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_storage_stype = getattr(torch, model_args.bnb_4bit_quant_storage_dtype)\n",
    "quant_storage_stype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_4bit_quant_type = model_args.bnb_4bit_quant_type\n",
    "bnb_4bit_quant_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_4bit_use_double_quant = model_args.use_nested_quant\n",
    "bnb_4bit_use_double_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if load_in_4bit:\n",
    "compute_dtype = getattr(torch, model_args.bnb_4bit_compute_dtype)\n",
    "compute_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=bnb_4bit_use_double_quant,\n",
    ")\n",
    "bnb_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### what does this `compute_type` do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the data type from the default `flaot32` to `bf16` to speed up computation. This requires cuda capability that supports `torch.bfloat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_dtype == torch.float16 and load_in_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\n",
    "            \"Your GPU supports bfloat16, you can accelerate training with the argument --bf16\"\n",
    "        )\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_capability()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### quantization type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_storage_stype "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_storage_stype.is_floating_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = quant_storage_stype if quant_storage_stype and quant_storage_stype.is_floating_point else torch.float32\n",
    "torch_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnb_4bit_quant_type = model_args.bnb_4bit_quant_type\n",
    "# bnb_4bit_quant_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NF4](https://huggingface.co/docs/transformers/main/en/quantization?bnb=4-bit) is a 4-bit data type adpated for weights initialized from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitsandbytes.nn import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've set the `compute_type` for bnb to be `torch.bloat16`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nested quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Nested quantization](https://huggingface.co/docs/transformers/main/en/quantization?bnb=4-bit) performs a second round of quantization on quantized weights to achieve additional 0.4 bits/parameter memory savings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnb_4bit_use_double_quant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Device Map (either 4bit or 8bit quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "if args.use_4bit_quantization or args.use_8bit_qunatization:\n",
    "    device_map = (\n",
    "        int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "        if torch.distributed.is_available() and torch.distributed.is_initialized()\n",
    "        else \"auto\"\n",
    "    )  # {\"\": 0}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ.get(\"LOCAL_RANK\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.distributed.is_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.distributed.is_initialized()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.distributed.is_initialized()` is false so the `device_map` is set to \"auto\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_map = (\n",
    "#     int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "#     if torch.distributed.is_available() and torch.distributed.is_initialized()\n",
    "#     else \"auto\"\n",
    "# )  # {\"\": 0}\n",
    "# device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `device_map` variable is used to determine the device mapping for distributed training when using quantization.\n",
    "\n",
    "In the context of distributed training, each process runs on a specific device (like a GPU). The `device_map` variable is used to specify which device the current process should run on.\n",
    "\n",
    "`int(os.environ.get(\"LOCAL_RANK\", -1))` tries to get the `LOCAL_RANK` environment variable, which is typically set in distributed training to indicate the rank of the current process. The rank is a unique identifier assigned to each process in a distributed training setup. If `LOCAL_RANK` is not set, it defaults to -1.\n",
    "\n",
    "`torch.distributed.is_available()` and `torch.distributed.is_initialized()` checks ensure that the PyTorch distributed package is available and has been initialized. If these conditions are met, it means the code is running in a distributed training setup.\n",
    "\n",
    "If `device_map` is set to \"auto\" during training, it'll automatically load the model on a GPU. \n",
    "\n",
    "When using the 8-bit quantized model, it is possible to [offload weights between the CPU and GPU](https://huggingface.co/docs/transformers/main/en/quantization?bnb=4-bit#offloading) with a custom `device_map` setting such as:\n",
    "\n",
    "```python\n",
    "device_map = {\n",
    "    \"transformer.word_embeddings\": 0,\n",
    "    \"transformer.word_embeddings_layernorm\": 0,\n",
    "    \"lm_head\": \"cpu\",\n",
    "    \"transformer.h\": 0,\n",
    "    \"transformer.ln_f\": 0,\n",
    "}\n",
    "```\n",
    "\n",
    "'0' represents the GPU. This allows support for very large models into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on whether `unsloth` is used, we use different methods to load the model. \n",
    "\n",
    "We also specify different attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.use_unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `unsloth` is not used, we initialize the model with `AutoModelForCausalLM`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if args.use_unsloth:\n",
    "    # Load model\n",
    "    model, _ = FastLanguageModel.from_pretrained(\n",
    "        model_name=args.model_name_or_path,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        dtype=None,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        load_in_8bit=load_in_8bit,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=device_map,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"flash_attention_2\" if args.use_flash_attn else \"eager\",\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also [quantization with bits and bytes](https://huggingface.co/docs/transformers/main/en/quantization?bnb=4-bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### flash attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [Flash Attention 2](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#Flash-Attention-2) in transformers can help speed up the training throughput. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.use_flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    load_in_8bit=load_in_8bit,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\" if args.use_flash_attn else \"eager\",\n",
    "    torch_dtype=torch_dtype,\n",
    ")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_args.model_name_or_path,\n",
    "#     load_in_8bit=load_in_8bit,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=device_map,\n",
    "#     trust_remote_code=True,\n",
    "#     attn_implementation=\"flash_attention_2\" if model_args.use_flash_attn else \"eager\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The memory footprint of the model is: {model.get_memory_footprint():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep peft_lora with quantization and no unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LORA PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter-Efficient Fine Tuning (PEFT) is a technique that allows you to fine-tune large models with limited resources. It does so by freezing the pretrained model parameters during fine-tuning, and add a small set of trainable parameters called adapters on top of it. Thus significantly [reduces the memory](https://huggingface.co/docs/transformers/model_memory_anatomy#anatomy-of-models-memory) required to fine-tune the model. \n",
    "\n",
    "Low-Rank Adaptation [(LoRA)](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora) is a popular adapter-based method. It represent the weight updates with two smaller 'update matrices' through low-rank decomposition. The original weight matrix is frozen but the \"update matrices\" are trained based on the new data. At the end, the original weights and the adapter weights are combined to create the new weights.\n",
    "\n",
    "Performance of LoRA fine-tuned models have been found to be comparable to that of fully fine-tuned models. Once the adapter weights are merged with the base model, it does not introduce additional inference latency.\n",
    "\n",
    "LoRA is othogonal to and can be combined with other PEFT methods. \n",
    "\n",
    "For fine-tunning transformer models, LoRA is typically applied to only attention blocks for simplicity. The number of parameters in the adapter is determined by the rank parameter `r` and the shape of the original weight matrix.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we are using 4-bit or 8-bit quantization for peft_lora and\n",
    "* We are NOT using unsloth \n",
    "\n",
    "Here is how we prepare for kbit training.\n",
    "\n",
    "```python\n",
    "if (\n",
    "    (args.use_4bit_quantization or args.use_8bit_qunatization)\n",
    "    and args.use_peft_lora\n",
    "    and not args.use_unsloth\n",
    "):\n",
    "    model = prepare_model_for_kbit_training(\n",
    "        model,\n",
    "        use_gradient_checkpointing=training_args.gradient_checkpointing,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": model_args.use_reentrant},\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_model_for_kbit_training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create peft model \n",
    "\n",
    "Depending on whether unsloth is used, we use different methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if args.use_peft_lora and not args.use_unsloth:\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        r=args.lora_r,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=args.lora_target_modules.split(\",\")\n",
    "        if args.lora_target_modules != \"all-linear\"\n",
    "        else args.lora_target_modules,\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "elif args.use_peft_lora and args.use_unsloth:\n",
    "    # Do model patching and add fast LoRA weights\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        r=args.lora_r,\n",
    "        target_modules=args.lora_target_modules.split(\",\")\n",
    "        if args.lora_target_modules != \"all-linear\"\n",
    "        else args.lora_target_modules,\n",
    "        use_gradient_checkpointing=training_args.gradient_checkpointing,\n",
    "        random_state=training_args.seed,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = None\n",
    "chat_template = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.use_peft_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.use_unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### lora_alpha\n",
    "\n",
    "Scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.lora_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.lora_dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### lora_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rank of the \"update matrices\" in int. Lower rank leads to smaller update matrices and fewer trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.lora_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether `bias` parameters should be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### target modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modules (e.g., attention blocks etc.) to which the LoRA weights are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.lora_target_modules.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=model_args.lora_alpha,\n",
    "    lora_dropout=model_args.lora_dropout,\n",
    "    r=model_args.lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=model_args.lora_target_modules.split(\",\")\n",
    "    if model_args.lora_target_modules != \"all-linear\"\n",
    "    else model_args.lora_target_modules,\n",
    ")\n",
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    special_tokens = None\n",
    "    chat_template = None\n",
    "    if args.chat_template_format == \"chatml\":\n",
    "        special_tokens = ChatmlSpecialTokens\n",
    "        chat_template = DEFAULT_CHATML_CHAT_TEMPLATE\n",
    "    elif args.chat_template_format == \"zephyr\":\n",
    "        special_tokens = ZephyrSpecialTokens\n",
    "        chat_template = DEFAULT_ZEPHYR_CHAT_TEMPLATE\n",
    "\n",
    "    if special_tokens is not None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            pad_token=special_tokens.pad_token.value,\n",
    "            bos_token=special_tokens.bos_token.value,\n",
    "            eos_token=special_tokens.eos_token.value,\n",
    "            additional_special_tokens=special_tokens.list(),\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        tokenizer.chat_template = chat_template\n",
    "        # make embedding resizing configurable?\n",
    "        model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            args.model_name_or_path, trust_remote_code=True\n",
    "        )\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = None\n",
    "chat_template = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.chat_template_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We call `get_peft_model` here** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure gradient checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient checkpointing is a technique used to reduce the memory usage when training deep learning models, at the cost of increased computation time. It's useful when training large models that would otherwise not fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    model.config.use_cache = not training_args.gradient_checkpointing\n",
    "```\n",
    "\n",
    "This line disables caching in the model configuration if gradient checkpointing is enabled. Caching is used to speed up computation by storing the results of expensive function calls and reusing them when the same inputs occur again. However, it increases memory usage, so it's disabled when gradient checkpointing is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = not training_args.gradient_checkpointing\n",
    "model.config.use_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "    training_args.gradient_checkpointing = (\n",
    "        training_args.gradient_checkpointing and not model_args.use_unsloth\n",
    "    )\n",
    "    if training_args.gradient_checkpointing:\n",
    "        training_args.gradient_checkpointing_kwargs = {\n",
    "            \"use_reentrant\": model_args.use_reentrant\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enable gradient checkpointing only if it was initially enabled and `use_unsloth` is not set in the model arguments.\n",
    "\n",
    "If gradient checkpointing is enabled, we set the `use_reentrant` argument according to the provided input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.gradient_checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.gradient_checkpointing and not model_args.use_unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.use_reentrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.gradient_checkpointing = (\n",
    "    training_args.gradient_checkpointing and not model_args.use_unsloth\n",
    ")\n",
    "if training_args.gradient_checkpointing:\n",
    "    training_args.gradient_checkpointing_kwargs = {\n",
    "        \"use_reentrant\": model_args.use_reentrant\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.gradient_checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.chat_template_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_chat_template=model_args.chat_template_format != \"none\"\n",
    "apply_chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args.splits.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args.dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in data_args.splits.split(\",\"):\n",
    "    try:\n",
    "        # Try first if dataset on a Hub repo\n",
    "        dataset = load_dataset(data_args.dataset_name, split=split)\n",
    "    except DatasetGenerationError:\n",
    "        # If not, check local dataset\n",
    "        dataset = load_from_disk(os.path.join(data_args.dataset_name, split))\n",
    "\n",
    "    if \"train\" in split:\n",
    "        raw_datasets[\"train\"] = dataset\n",
    "    elif \"test\" in split:\n",
    "        raw_datasets[\"test\"] = dataset\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Split type {split} not recognized as one of test or train.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    if apply_chat_template:\n",
    "        raw_datasets = raw_datasets.map(\n",
    "            preprocess,\n",
    "            batched=True,\n",
    "            remove_columns=raw_datasets[\"train\"].column_names,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = raw_datasets[\"train\"]\n",
    "valid_data = raw_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Size of the train set: {len(train_data)}. Size of the validation set: {len(valid_data)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"A sample of train dataset: {train_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_data\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = valid_data\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_column = data_args.dataset_text_field\n",
    "data_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review all the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss those parameters that we have not yet covered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size is recommended to be 2^N, often muliple of 8.\n",
    "\n",
    "[Tensor Core Requirements](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc) define the multiplier based on the dtype and the hardware. For instance, \n",
    "* for fp16 data type a multiple of 8 is recommended \n",
    "* but for an A100 GPU, a multiples of 64 is recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gradient accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient accumulation is a technique designed to compute gradients in smaller, more manageable increments rather than processing the entire batch simultaneously. This method involves a series of forward and backward passes through the model, during which gradients are calculated and accumulated. After a sufficient number of gradients have been gathered, the optimization step of the model is carried out. \n",
    "\n",
    "The advantage of using gradient accumulation is that it allows for an increase in the effective batch size, surpassing the constraints set by the GPU's memory. However, it's crucial to be aware that the extra forward and backward passes required by this method can potentially decelerate the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.per_device_train_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results in a 4x2 = 8 effective batch size on a single GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gradient checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient checkpointing is a technique that balances memory usage and computational speed during model training. Instead of storing all activations from the forward pass for gradient computation, which can consume significant memory, or discarding and recalculating them, which can slow down training, gradient checkpointing selectively saves certain activations. This means only a subset of activations need to be recalculated, optimizing both memory and computation resources.\n",
    "\n",
    "But it comes with a cost of [slowing down the training by approximately 20%](https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.gradient_checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixed precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed precision training is a method that enhances computational efficiency in model training by using lower-precision numerical formats for certain variables. While most models traditionally use 32-bit floating point precision (fp32), not all variables need this level of precision. By lowering the precision of some variables to formats like 16-bit floating point (fp16), computations can be sped up.\n",
    "\n",
    "Typically in mixed precision training: \n",
    "* Activations are in half precision (fp16)\n",
    "* Despite gradients being computed in half precision, they are converted back to full precision for optimization, so no memory is saved in this step. \n",
    "* It could also lead to more GPU memory being utilized, especially for small batch sizes. \n",
    "\n",
    "Newer GPU architectures, like the Ampere architecture, offer bf16 and tf32 data types. Tradditonal one is ft16.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_args.tf32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_args.bf16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    # peft_config=peft_config, disable peft_config here since we already created the peft_model\n",
    "    packing=data_args.packing,\n",
    "    dataset_kwargs={\n",
    "        \"append_concat_token\": data_args.append_concat_token,\n",
    "        \"add_special_tokens\": data_args.add_special_tokens,\n",
    "    },\n",
    "    dataset_text_field=data_args.dataset_text_field,\n",
    "    max_seq_length=data_args.max_seq_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.accelerator.print(f\"{trainer.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.use_peft_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_args.use_peft_lora:\n",
    "    trainer.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loftq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For QLoRA training, when we're preparing to quantize the base model, it's worth considering the use of LoftQ initialization. This method has demonstrated its ability to enhance performance in conjunction with quantization. The underlying concept is to initialize the LoRA weights in a way that minimizes the quantization error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.use_loftq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoftQ initialization when using QLoRA\n",
    "if model_args.use_4bit_quantization and model_args.use_loftq:\n",
    "    loftq_init(trainer.model, tokenizer, train_dataset, data_args.max_seq_length ,model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If enabled, `loftq_init` will call `replace_lora_weights_loftq` to replace the LoRA weights with LoftQ-initialized weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_args.resume_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = None\n",
    "if training_args.resume_from_checkpoint is not None:\n",
    "    checkpoint = training_args.resume_from_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.is_fsdp_enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trainer.is_fsdp_enabled:\n",
    "    trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n",
    "\n",
    "#trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
