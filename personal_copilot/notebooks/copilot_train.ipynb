{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create a virtual environment and install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "cat /etc/os-release\n",
    "nvcc -V\n",
    "cd ../personal_copilot\n",
    "python3.11 -m venv .copilot\n",
    "source .copilot/bin/activate\n",
    "pip install --upgrade pip setuptools wheel\n",
    "pip install torch torchvision torchaudio\n",
    "pip install packaging\n",
    "pip install flash-attn\n",
    "pip install -r training/requirements.txt\n",
    "pip install -r dateset_generation/requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow `personal_copilot/README.md`. \n",
    "\n",
    "```shell\n",
    "export GH_ACCESS_TOKEN=xxxx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"../dataset_generation\")\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python clone_hf_repos.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls hf_public_repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run data processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could collate and push to hub.\n",
    "\n",
    "```shell\n",
    "python prepare_hf_dataset.py\n",
    "```\n",
    "\n",
    "we can also just download it from the hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "```shell\n",
    "python train.py \\\n",
    "    --model_name_or_path \"bigcode/starcoder2-7b\" \\\n",
    "    --lora_r 32 \\\n",
    "    --lora_alpha 64 \\\n",
    "    --lora_dropout 0.0 \\\n",
    "    --lora_target_modules \"c_proj,c_attn,q_attn,c_fc,c_proj\" \\\n",
    "    --use_nested_quant \\\n",
    "    --bnb_4bit_compute_dtype \"bfloat16\" \\\n",
    "    --use_flash_attn \\\n",
    "    --use_peft_lora \\\n",
    "    --use_4bit_quantization \\\n",
    "    --dataset_name \"smangrul/hug_stack\" \\\n",
    "    --dataset_text_field \"text\" \\\n",
    "    --max_seq_length 1024 \\\n",
    "    --fim_rate 0.5 \\\n",
    "    --fim_spm_rate 0.5 \\\n",
    "    --splits \"train\" \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --bf16 \\\n",
    "    --learning_rate 5e-4 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --max_steps 1000 \\\n",
    "    --warmup_steps 30 \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --evaluation_strategy \"steps\" \\\n",
    "    --eval_steps 50 \\\n",
    "    --save_steps 50 \\\n",
    "    --logging_steps 25 \\\n",
    "    --output_dir \"peft-lora-starcoder2-7b-personal-copilot-dual-3090-local\" \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training is interrupted, we can resume it by adding `--resume_from_checkpoint \"path/to/checkpoint\"`.\n",
    "\n",
    "```shell\n",
    "    python train.py \\\n",
    "    --model_name_or_path \"bigcode/starcoder2-7b\" \\\n",
    "    --lora_r 32 \\\n",
    "    --lora_alpha 64 \\\n",
    "    --lora_dropout 0.0 \\\n",
    "    --lora_target_modules \"c_proj,c_attn,q_attn,c_fc,c_proj\" \\\n",
    "    --use_nested_quant \\\n",
    "    --bnb_4bit_compute_dtype \"bfloat16\" \\\n",
    "    --use_flash_attn \\\n",
    "    --use_peft_lora \\\n",
    "    --use_4bit_quantization \\\n",
    "    --dataset_name \"smangrul/hug_stack\" \\\n",
    "    --dataset_text_field \"text\" \\\n",
    "    --max_seq_length 1024 \\\n",
    "    --fim_rate 0.5 \\\n",
    "    --fim_spm_rate 0.5 \\\n",
    "    --splits \"train\" \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --bf16 \\\n",
    "    --learning_rate 5e-4 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --max_steps 1000 \\\n",
    "    --warmup_steps 30 \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --evaluation_strategy \"steps\" \\\n",
    "    --eval_steps 50 \\\n",
    "    --save_steps 50 \\\n",
    "    --logging_steps 25 \\\n",
    "    --output_dir \"peft-lora-starcoder2-7b-personal-copilot-dual-3090-local\" \\\n",
    "    --resume_from_checkpoint \"peft-lora-starcoder2-7b-personal-copilot-dual-3090-local/checkpoint-450\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Tensorboard\n",
    "\n",
    "```shell\n",
    "cd personal_copilot/training/peft-lora-starcoder2-7b-personal-copilot-dual-3090-local\n",
    "tensorboard --logdir=runs --bind_all\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can run the training, let's go back to understand what is actually going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the parent directory to the path\n",
    "sys.path.append('../training')\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = ['ipywidgets']  # Add your packages here\n",
    "\n",
    "for package in packages:\n",
    "    !pip show {package} > /dev/null || pip install {package}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'code_copilot.ipynb'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import random\n",
    "import sys\n",
    "import humanfriendly\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import IterableDataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import fim\n",
    "from train import ModelArguments, DataTrainingArguments, chars_token_ratio, ConstantLengthDataset, create_datasets, create_and_prepare_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with defining a `HfArgumentParser`: This module from the Hugging Face transformers library parses command-line arguments related to the model, data, and training configurations. \n",
    "\n",
    "* We can place all the arguments in a `json` file and use `parse_json_file`.\n",
    "* or place them in the command line and use `parse_args_into_dataclasses`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs \n",
    "\n",
    "#### from command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\n",
    "    \"--model_name_or_path\", \"bigcode/starcoder2-7b\",\n",
    "    \"--lora_r\", \"32\",\n",
    "    \"--lora_alpha\", \"64\",\n",
    "    \"--lora_dropout\", \"0.0\",\n",
    "    \"--lora_target_modules\", \"c_proj,c_attn,q_attn,c_fc,c_proj\",\n",
    "    \"--use_nested_quant\",\n",
    "    \"--bnb_4bit_compute_dtype\", \"bfloat16\",\n",
    "    \"--use_flash_attn\",\n",
    "    \"--use_peft_lora\",\n",
    "    \"--use_4bit_quantization\",\n",
    "    \"--dataset_name\", \"smangrul/hug_stack\",\n",
    "    \"--dataset_text_field\", \"text\",\n",
    "    \"--max_seq_length\", \"1024\",\n",
    "    \"--fim_rate\", \"0.5\",\n",
    "    \"--fim_spm_rate\", \"0.5\",\n",
    "    \"--splits\", \"train\",\n",
    "    \"--per_device_train_batch_size\", \"2\",\n",
    "    \"--per_device_eval_batch_size\", \"2\",\n",
    "    \"--gradient_accumulation_steps\", \"4\",\n",
    "    \"--bf16\",\n",
    "    \"--learning_rate\", \"5e-4\",\n",
    "    \"--lr_scheduler_type\", \"cosine\",\n",
    "    \"--weight_decay\", \"0.01\",\n",
    "    \"--max_steps\", \"1000\",\n",
    "    \"--warmup_steps\", \"30\",\n",
    "    \"--dataloader_num_workers\", \"4\",\n",
    "    \"--eval_strategy\", \"steps\",\n",
    "    \"--eval_steps\", \"50\",\n",
    "    \"--save_steps\", \"50\",\n",
    "    \"--logging_steps\", \"25\",\n",
    "    \"--output_dir\", \"peft-lora-starcoder2-7b-personal-copilot-test\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse arguments\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelArguments(model_name_or_path='bigcode/starcoder2-7b', lora_alpha=64, lora_dropout=0.0, lora_r=32, lora_target_modules='c_proj,c_attn,q_attn,c_fc,c_proj', use_nested_quant=True, bnb_4bit_compute_dtype='bfloat16', bnb_4bit_quant_type='nf4', use_flash_attn=True, use_peft_lora=True, use_8bit_qunatization=False, use_4bit_quantization=True, use_reentrant=False, use_unsloth=False, use_loftq=False, use_loftq_callback=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTrainingArguments(dataset_name='smangrul/hug_stack', dataset_text_field='text', max_seq_length=1024, test_size=0.1, fim_rate=0.5, fim_spm_rate=0.5, splits='train')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use JSON to get input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_json_path = \"data/copilot_train_input.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/charles/github/LLM-Workshop/personal_copilot/notebooks'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_json_file(json_file=input_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelArguments(model_name_or_path='bigcode/starcoder2-7b', lora_alpha=64, lora_dropout=0.0, lora_r=32, lora_target_modules='c_proj,c_attn,q_attn,c_fc,c_proj', use_nested_quant=True, bnb_4bit_compute_dtype='bfloat16', bnb_4bit_quant_type='nf4', use_flash_attn=True, use_peft_lora=True, use_8bit_qunatization=False, use_4bit_quantization=True, use_reentrant=False, use_unsloth=False, use_loftq=False, use_loftq_callback=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTrainingArguments(dataset_name='smangrul/hug_stack', dataset_text_field='text', max_seq_length=1024, test_size=0.1, fim_rate=0.5, fim_spm_rate=0.5, splits='train')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'peft-lora-starcoder2-7b-personal-copilot-test'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bigcode/starcoder2-7b'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='bigcode/starcoder2-7b', vocab_size=49152, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<|endoftext|>', '<fim_prefix>', '<fim_middle>', '<fim_suffix>', '<fim_pad>', '<repo_name>', '<file_sep>', '<issue_start>', '<issue_comment>', '<issue_closed>', '<jupyter_start>', '<jupyter_text>', '<jupyter_code>', '<jupyter_output>', '<jupyter_script>', '<empty_output>', '<code_to_intermediate>', '<intermediate_to_code>', '<pr>', '<pr_status>', '<pr_is_merged>', '<pr_base>', '<pr_file>', '<pr_base_code>', '<pr_diff>', '<pr_diff_hunk>', '<pr_comment>', '<pr_event_id>', '<pr_review>', '<pr_review_state>', '<pr_review_comment>', '<pr_in_reply_to_review_id>', '<pr_in_reply_to_comment_id>', '<pr_diff_hunk_comment_line>', '<NAME>', '<EMAIL>', '<KEY>', '<PASSWORD>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<fim_prefix>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<fim_middle>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<fim_suffix>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"<fim_pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"<repo_name>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"<file_sep>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t7: AddedToken(\"<issue_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t8: AddedToken(\"<issue_comment>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t9: AddedToken(\"<issue_closed>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t10: AddedToken(\"<jupyter_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t11: AddedToken(\"<jupyter_text>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t12: AddedToken(\"<jupyter_code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t13: AddedToken(\"<jupyter_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t14: AddedToken(\"<jupyter_script>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t15: AddedToken(\"<empty_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t16: AddedToken(\"<code_to_intermediate>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t17: AddedToken(\"<intermediate_to_code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t18: AddedToken(\"<pr>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t19: AddedToken(\"<pr_status>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20: AddedToken(\"<pr_is_merged>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t21: AddedToken(\"<pr_base>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t22: AddedToken(\"<pr_file>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t23: AddedToken(\"<pr_base_code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t24: AddedToken(\"<pr_diff>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t25: AddedToken(\"<pr_diff_hunk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t26: AddedToken(\"<pr_comment>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t27: AddedToken(\"<pr_event_id>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t28: AddedToken(\"<pr_review>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t29: AddedToken(\"<pr_review_state>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30: AddedToken(\"<pr_review_comment>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t31: AddedToken(\"<pr_in_reply_to_review_id>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32: AddedToken(\"<pr_in_reply_to_comment_id>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t33: AddedToken(\"<pr_diff_hunk_comment_line>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t34: AddedToken(\"<NAME>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t35: AddedToken(\"<EMAIL>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t36: AddedToken(\"<KEY>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t37: AddedToken(\"<PASSWORD>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_name': 'smangrul/hug_stack',\n",
       " 'dataset_text_field': 'text',\n",
       " 'max_seq_length': 1024,\n",
       " 'test_size': 0.1,\n",
       " 'fim_rate': 0.5,\n",
       " 'fim_spm_rate': 0.5,\n",
       " 'splits': 'train'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(data_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = training_args.seed\n",
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'smangrul/hug_stack'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args.dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'id', 'metadata', '__index_level_0__'],\n",
       "    num_rows: 6579\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(data_args.dataset_name, split=data_args.splits)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = data_args.test_size\n",
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'id', 'metadata', '__index_level_0__'],\n",
       "        num_rows: 5921\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'id', 'metadata', '__index_level_0__'],\n",
       "        num_rows: 658\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(\n",
    "    test_size=test_size, seed=seed, shuffle=True\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'id', 'metadata', '__index_level_0__'],\n",
       "    num_rows: 5921\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = dataset[\"train\"]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'id', 'metadata', '__index_level_0__'],\n",
       "    num_rows: 658\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data = dataset[\"test\"]\n",
    "valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train set: 5921. Size of the validation set: 658\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Size of the train set: {len(train_data)}. Size of the validation set: {len(valid_data)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_column = data_args.dataset_text_field\n",
    "data_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the number of tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_tokens(dataset, tokenizer, data_column):\n",
    "    \"\"\"\n",
    "    Compute the total number of tokens in the dataset.\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    for example in tqdm(dataset):\n",
    "        total_tokens += len(tokenizer(example[data_column]).tokens())\n",
    "\n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_tokens_train = total_tokens(train_data, tokenizer, data_column)\n",
    "# total_tokens_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cache the results since it takes time to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a memory object for caching\n",
    "import shutil\n",
    "cache_dir = 'data/cache/total_tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if we want to delete the cache\n",
    "# if os.path.exists(cache_dir):\n",
    "#     shutil.rmtree(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "memory = Memory(cache_dir, verbose=0)\n",
    "\n",
    "@memory.cache\n",
    "def total_tokens(dataset, tokenizer, data_column):\n",
    "    \"\"\"\n",
    "    Compute the total number of tokens in the dataset.\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    for example in tqdm(dataset):\n",
    "        total_tokens += len(tokenizer(example[data_column]).tokens())\n",
    "    \n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23328978"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens_train = total_tokens(train_data, tokenizer, data_column)\n",
    "total_tokens_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_tokens_train = humanfriendly.format_number(total_tokens_train)\n",
    "# total_tokens_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of tokens in the training dataset is: 23,328,978\n"
     ]
    }
   ],
   "source": [
    "print(f\"The total number of tokens in the training dataset is: {total_tokens_train:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total of 23M tokens in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chars_token_ratio(dataset, tokenizer, data_column, nb_examples=400):\n",
    "    \"\"\"\n",
    "    Estimate the average number of characters per token in the dataset.\n",
    "    \"\"\"\n",
    "    total_characters, total_tokens = 0, 0\n",
    "    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n",
    "        total_characters += len(example[data_column])\n",
    "        total_tokens += len(tokenizer(example[data_column]).tokens())\n",
    "\n",
    "    return total_characters / total_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:01<00:00, 212.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.6223575039906772"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_per_token = chars_token_ratio(train_data, tokenizer, data_column)\n",
    "chars_per_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The character to token ratio of the dataset is: 3.62\n"
     ]
    }
   ],
   "source": [
    "print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset, eval_dataset = create_datasets(\n",
    "#     tokenizer, data_args, training_args.seed\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mConstantLengthDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minfinite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_of_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mchars_per_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcontent_field\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfim_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfim_spm_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m Initialize self.  See help(type(self)) for accurate signature.\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minfinite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mnum_of_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mchars_per_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcontent_field\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfim_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfim_spm_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfinite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfinite\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_buffer_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchars_per_token\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_of_sequences\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_field\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent_field\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfim_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfim_rate\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfim_spm_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfim_spm_rate\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos_token_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix_tok_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix_tok_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmiddle_tok_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_tok_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fim_token_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix_tok_id\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfim_rate\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FIM is not supported by tokenizer, disabling FIM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfim_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/github/LLM-Workshop/personal_copilot/training/train.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "ConstantLengthDataset.__init__??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = data_args.max_seq_length\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fim_rate = data_args.fim_rate\n",
    "fim_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fim_spm_rate = data_args.fim_spm_rate\n",
    "fim_spm_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample of train dataset: {'input_ids': tensor([   63, 20455,    53,  ...,    45,  1612,    46]), 'labels': tensor([   63, 20455,    53,  ...,    45,  1612,    46])}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ConstantLengthDataset(\n",
    "    tokenizer,\n",
    "    train_data,\n",
    "    infinite=True,\n",
    "    seq_length=max_seq_length,\n",
    "    chars_per_token=chars_per_token,\n",
    "    content_field=data_column,\n",
    "    fim_rate=fim_rate,\n",
    "    fim_spm_rate=fim_spm_rate,\n",
    "    seed=seed,\n",
    "    shuffle=True,\n",
    ")\n",
    "train_dataset\n",
    "print(f\"A sample of train dataset: {next(iter(train_dataset))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample of valid dataset: {'input_ids': tensor([   40, 10633,    66,  ...,  6878,    49,   327]), 'labels': tensor([   40, 10633,    66,  ...,  6878,    49,   327])}\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = ConstantLengthDataset(\n",
    "    tokenizer,\n",
    "    valid_data,\n",
    "    infinite=False,\n",
    "    seq_length=max_seq_length,\n",
    "    chars_per_token=chars_per_token,\n",
    "    content_field=data_column,\n",
    "    fim_rate=fim_rate,\n",
    "    fim_spm_rate=fim_spm_rate,\n",
    "    seed=seed,\n",
    ")\n",
    "print(f\"A sample of valid dataset: {next(iter(eval_dataset))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.start_iteration = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConstantLengthDataset deepdive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_map = None\n",
    "bnb_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_in_8bit = model_args.use_8bit_qunatization\n",
    "load_in_8bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.use_unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_args.use_unsloth:\n",
    "    from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_in_4bit = model_args.use_4bit_quantization\n",
    "load_in_4bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization & bnb config\n",
    "\n",
    "We are using [QLoRA](https://huggingface.co/papers/2305.14314). QLoRA is a method for fine-tuning models that employs a two-pronged approach. \n",
    "\n",
    "Firstly, it quantizes the model to 4-bits, thereby reducing the computational resources required. \n",
    "\n",
    "Secondly, it incorporates a set of Low-Rank Adaptation (LoRA) weights into the model, which are fine-tuned via the quantized weights. \n",
    "\n",
    "In addition to the conventional Float4 data type (LinearFP4), QLoRA introduces a new 4-bit NormalFloat (LinearNF4) data type. This new data type is specifically designed for quantizing normally distributed data, and can enhance the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bfloat16'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_4bit_compute_dtype = model_args.bnb_4bit_compute_dtype\n",
    "bnb_4bit_compute_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nf4'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_4bit_quant_type = model_args.bnb_4bit_quant_type\n",
    "bnb_4bit_quant_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_4bit_use_double_quant = model_args.use_nested_quant\n",
    "bnb_4bit_use_double_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if load_in_4bit:\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "compute_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BitsAndBytesConfig {\n",
       "  \"_load_in_4bit\": true,\n",
       "  \"_load_in_8bit\": false,\n",
       "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
       "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "  \"bnb_4bit_quant_type\": \"nf4\",\n",
       "  \"bnb_4bit_use_double_quant\": true,\n",
       "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "  \"llm_int8_has_fp16_weight\": false,\n",
       "  \"llm_int8_skip_modules\": null,\n",
       "  \"llm_int8_threshold\": 6.0,\n",
       "  \"load_in_4bit\": true,\n",
       "  \"load_in_8bit\": false,\n",
       "  \"quant_method\": \"bitsandbytes\"\n",
       "}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=bnb_4bit_use_double_quant,\n",
    ")\n",
    "bnb_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### what does this `compute_type` do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the data type from the default `flaot32` to `bf16` to speed up computation. This requires cuda capability that supports `torch.bfloat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_dtype == torch.float16 and load_in_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\n",
    "            \"Your GPU supports bfloat16, you can accelerate training with the argument --bf16\"\n",
    "        )\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 6)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_capability()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### quantization type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nf4'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_4bit_quant_type = model_args.bnb_4bit_quant_type\n",
    "bnb_4bit_quant_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NF4](https://huggingface.co/docs/transformers/main/en/quantization?bnb=4-bit) is a 4-bit data type adpated for weights initialized from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitsandbytes.nn import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear4bit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcompute_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcompress_statistics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mquant_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fp4'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mquant_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minput_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moutput_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcompute_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcompress_statistics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mquant_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fp4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mquant_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Initialize Linear4bit class.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            input_features (`str`):\u001b[0m\n",
      "\u001b[0;34m                Number of input features of the linear layer.\u001b[0m\n",
      "\u001b[0;34m            output_features (`str`):\u001b[0m\n",
      "\u001b[0;34m                Number of output features of the linear layer.\u001b[0m\n",
      "\u001b[0;34m            bias (`bool`, defaults to `True`):\u001b[0m\n",
      "\u001b[0;34m                Whether the linear class uses the bias term as well.\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParams4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcompress_statistics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompress_statistics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mquant_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mquant_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_storage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# self.persistent_buffers = []  # TODO consider as way to save quant state\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_dtype\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_type_is_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/github/LLM-Workshop/personal_copilot/.copilot/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "modules.Linear4bit.__init__??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear4bit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_compute_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "    \u001b[0;32mdef\u001b[0m \u001b[0mset_compute_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# the input is in a dtype that is safe to compute in, we switch\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# to this type for speed and stability\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# we take the compoute dtype passed into the layer\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# single batch inference with input torch.float16 and compute_dtype float32 -> slow inference when it could be fast\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# warn the user about this\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\"Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".*inference.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\"Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".*inference or training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/github/LLM-Workshop/personal_copilot/.copilot/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "modules.Linear4bit.set_compute_type??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've set the `compute_type` for bnb to be `torch.bloat16`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nested quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Nested quantization](https://huggingface.co/docs/transformers/main/en/quantization?bnb=4-bit) performs a second round of quantization on quantized weights to achieve additional 0.4 bits/parameter memory savings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_4bit_use_double_quant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Device Map (either 4bit or 8bit quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "if args.use_4bit_quantization or args.use_8bit_qunatization:\n",
    "    device_map = (\n",
    "        int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "        if torch.distributed.is_available() and torch.distributed.is_initialized()\n",
    "        else \"auto\"\n",
    "    )  # {\"\": 0}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get(\"LOCAL_RANK\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.distributed.is_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.distributed.is_initialized()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.distributed.is_initialized()` is false so the `device_map` is set to \"auto\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'auto'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_map = (\n",
    "    int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    if torch.distributed.is_available() and torch.distributed.is_initialized()\n",
    "    else \"auto\"\n",
    ")  # {\"\": 0}\n",
    "device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `device_map` variable is used to determine the device mapping for distributed training when using quantization.\n",
    "\n",
    "In the context of distributed training, each process runs on a specific device (like a GPU). The `device_map` variable is used to specify which device the current process should run on.\n",
    "\n",
    "`int(os.environ.get(\"LOCAL_RANK\", -1))` tries to get the `LOCAL_RANK` environment variable, which is typically set in distributed training to indicate the rank of the current process. The rank is a unique identifier assigned to each process in a distributed training setup. If `LOCAL_RANK` is not set, it defaults to -1.\n",
    "\n",
    "`torch.distributed.is_available()` and `torch.distributed.is_initialized()` checks ensure that the PyTorch distributed package is available and has been initialized. If these conditions are met, it means the code is running in a distributed training setup.\n",
    "\n",
    "If `device_map` is set to \"auto\" during training, it'll automatically load the model on a GPU. \n",
    "\n",
    "When using the 8-bit quantized model, it is possible to [offload weights between the CPU and GPU](https://huggingface.co/docs/transformers/main/en/quantization?bnb=4-bit#offloading) with a custom `device_map` setting such as:\n",
    "\n",
    "```python\n",
    "device_map = {\n",
    "    \"transformer.word_embeddings\": 0,\n",
    "    \"transformer.word_embeddings_layernorm\": 0,\n",
    "    \"lm_head\": \"cpu\",\n",
    "    \"transformer.h\": 0,\n",
    "    \"transformer.ln_f\": 0,\n",
    "}\n",
    "```\n",
    "\n",
    "'0' represents the GPU. This allows support for very large models into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on whether `unsloth` is used, we use different methods to load the model. \n",
    "\n",
    "We also specify different attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.use_unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `unsloth` is not used, we initialize the model with `AutoModelForCausalLM`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if args.use_unsloth:\n",
    "    # Load model\n",
    "    model, _ = FastLanguageModel.from_pretrained(\n",
    "        model_name=args.model_name_or_path,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        dtype=None,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        load_in_8bit=load_in_8bit,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=device_map,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"flash_attention_2\" if args.use_flash_attn else \"eager\",\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bigcode/starcoder2-7b'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BitsAndBytesConfig {\n",
       "  \"_load_in_4bit\": true,\n",
       "  \"_load_in_8bit\": false,\n",
       "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
       "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "  \"bnb_4bit_quant_type\": \"nf4\",\n",
       "  \"bnb_4bit_use_double_quant\": true,\n",
       "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "  \"llm_int8_has_fp16_weight\": false,\n",
       "  \"llm_int8_skip_modules\": null,\n",
       "  \"llm_int8_threshold\": 6.0,\n",
       "  \"load_in_4bit\": true,\n",
       "  \"load_in_8bit\": false,\n",
       "  \"quant_method\": \"bitsandbytes\"\n",
       "}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also [quantization with bits and bytes](https://huggingface.co/docs/transformers/main/en/quantization?bnb=4-bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'auto'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### flash attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [Flash Attention 2](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#Flash-Attention-2) in transformers can help speed up the training throughput. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.use_flash_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are using flash attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddcab97ee2041d488b4b44e2b303c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    load_in_8bit=load_in_8bit,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\" if model_args.use_flash_attn else \"eager\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Starcoder2ForCausalLM(\n",
       "  (model): Starcoder2Model(\n",
       "    (embed_tokens): Embedding(49152, 4608)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Starcoder2DecoderLayer(\n",
       "        (self_attn): Starcoder2FlashAttention2(\n",
       "          (q_proj): Linear4bit(in_features=4608, out_features=4608, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=4608, out_features=512, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=4608, out_features=512, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=4608, out_features=4608, bias=True)\n",
       "          (rotary_emb): Starcoder2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Starcoder2MLP(\n",
       "          (c_fc): Linear4bit(in_features=4608, out_features=18432, bias=True)\n",
       "          (c_proj): Linear4bit(in_features=18432, out_features=4608, bias=True)\n",
       "          (act): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4608,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4608,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((4608,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4608, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory footprint of the model is: 4,197,640,192\n"
     ]
    }
   ],
   "source": [
    "print(f\"The memory footprint of the model is: {model.get_memory_footprint():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is 4.4G."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep peft_lora with quantization and no unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LORA PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter-Efficient Fine Tuning (PEFT) is a technique that allows you to fine-tune large models with limited resources. It does so by freezing the pretrained model parameters during fine-tuning, and add a small set of trainable parameters called adapters on top of it. Thus significantly [reduces the memory](https://huggingface.co/docs/transformers/model_memory_anatomy#anatomy-of-models-memory) required to fine-tune the model. \n",
    "\n",
    "Low-Rank Adaptation [(LoRA)](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora) is a popular adapter-based method. It represent the weight updates with two smaller 'update matrices' through low-rank decomposition. The original weight matrix is frozen but the \"update matrices\" are trained based on the new data. At the end, the original weights and the adapter weights are combined to create the new weights.\n",
    "\n",
    "Performance of LoRA fine-tuned models have been found to be comparable to that of fully fine-tuned models. Once the adapter weights are merged with the base model, it does not introduce additional inference latency.\n",
    "\n",
    "LoRA is othogonal to and can be combined with other PEFT methods. \n",
    "\n",
    "For fine-tunning transformer models, LoRA is typically applied to only attention blocks for simplicity. The number of parameters in the adapter is determined by the rank parameter `r` and the shape of the original weight matrix.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we are using 4-bit or 8-bit quantization for peft_lora and\n",
    "* We are NOT using unsloth \n",
    "\n",
    "Here is how we prepare for kbit training.\n",
    "\n",
    "```python\n",
    "if (\n",
    "    (args.use_4bit_quantization or args.use_8bit_qunatization)\n",
    "    and args.use_peft_lora\n",
    "    and not args.use_unsloth\n",
    "):\n",
    "    model = prepare_model_for_kbit_training(\n",
    "        model,\n",
    "        use_gradient_checkpointing=training_args.gradient_checkpointing,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": model_args.use_reentrant},\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mprepare_model_for_kbit_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_gradient_checkpointing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgradient_checkpointing_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Note this method only works for `transformers` models.\n",
      "\n",
      "This method wraps the entire protocol for preparing a model before running a training. This includes:\n",
      "    1- Cast the layernorm in fp32 2- making output embedding layer require grads 3- Add the upcasting of the lm\n",
      "    head to fp32\n",
      "\n",
      "Args:\n",
      "    model (`transformers.PreTrainedModel`):\n",
      "        The loaded model from `transformers`\n",
      "    use_gradient_checkpointing (`bool`, *optional*, defaults to `True`):\n",
      "        If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
      "    gradient_checkpointing_kwargs (`dict`, *optional*, defaults to `None`):\n",
      "        Keyword arguments to pass to the gradient checkpointing function, please refer to the documentation of\n",
      "        `torch.utils.checkpoint.checkpoint` for more details about the arguments that you can pass to that method.\n",
      "        Note this is only available in the latest transformers versions (> 4.34.1).\n",
      "\u001b[0;31mFile:\u001b[0m      ~/github/LLM-Workshop/personal_copilot/.copilot/lib/python3.11/site-packages/peft/utils/other.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "prepare_model_for_kbit_training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create peft model \n",
    "\n",
    "Depending on whether unsloth is used, we use different methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if args.use_peft_lora and not args.use_unsloth:\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        r=args.lora_r,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=args.lora_target_modules.split(\",\")\n",
    "        if args.lora_target_modules != \"all-linear\"\n",
    "        else args.lora_target_modules,\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "elif args.use_peft_lora and args.use_unsloth:\n",
    "    # Do model patching and add fast LoRA weights\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        r=args.lora_r,\n",
    "        target_modules=args.lora_target_modules.split(\",\")\n",
    "        if args.lora_target_modules != \"all-linear\"\n",
    "        else args.lora_target_modules,\n",
    "        use_gradient_checkpointing=training_args.gradient_checkpointing,\n",
    "        random_state=training_args.seed,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.use_peft_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.use_unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### lora_alpha\n",
    "\n",
    "Scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.lora_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.lora_dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### lora_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rank of the \"update matrices\" in int. Lower rank leads to smaller update matrices and fewer trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.lora_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether `bias` parameters should be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### target modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modules (e.g., attention blocks etc.) to which the LoRA weights are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c_proj', 'c_attn', 'q_attn', 'c_fc', 'c_proj']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.lora_target_modules.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=32, target_modules={'q_attn', 'c_attn', 'c_proj', 'c_fc'}, lora_alpha=64, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=model_args.lora_alpha,\n",
    "    lora_dropout=model_args.lora_dropout,\n",
    "    r=model_args.lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=model_args.lora_target_modules.split(\",\")\n",
    "    if model_args.lora_target_modules != \"all-linear\"\n",
    "    else model_args.lora_target_modules,\n",
    ")\n",
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'peft_type': <PeftType.LORA: 'LORA'>,\n",
       " 'auto_mapping': None,\n",
       " 'base_model_name_or_path': None,\n",
       " 'revision': None,\n",
       " 'task_type': 'CAUSAL_LM',\n",
       " 'inference_mode': False,\n",
       " 'r': 32,\n",
       " 'target_modules': {'c_attn', 'c_fc', 'c_proj', 'q_attn'},\n",
       " 'lora_alpha': 64,\n",
       " 'lora_dropout': 0.0,\n",
       " 'fan_in_fan_out': False,\n",
       " 'bias': 'none',\n",
       " 'use_rslora': False,\n",
       " 'modules_to_save': None,\n",
       " 'init_lora_weights': True,\n",
       " 'layers_to_transform': None,\n",
       " 'layers_pattern': None,\n",
       " 'rank_pattern': {},\n",
       " 'alpha_pattern': {},\n",
       " 'megatron_config': None,\n",
       " 'megatron_core': 'megatron.core',\n",
       " 'loftq_config': {},\n",
       " 'use_dora': False,\n",
       " 'layer_replication': None}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mLoraConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpeft_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPeftType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mauto_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbase_model_name_or_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrevision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtask_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTaskType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minference_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtarget_modules\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[Union[list[str], str]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlora_alpha\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlora_dropout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'float'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfan_in_fan_out\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Literal['none', 'all', 'lora_only']\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_rslora\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodules_to_save\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[list[str]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minit_lora_weights\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"bool | Literal['gaussian', 'loftq']\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlayers_to_transform\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[Union[list[int], int]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlayers_pattern\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[Union[list[str], str]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrank_pattern\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[dict]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0malpha_pattern\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[dict]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmegatron_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[dict]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmegatron_core\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[str]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'megatron.core'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mloftq_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Union[LoftQConfig, dict]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_dora\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlayer_replication\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[list[tuple[int, int]]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "This is the configuration class to store the configuration of a [`LoraModel`].\n",
      "\n",
      "Args:\n",
      "    r (`int`):\n",
      "        Lora attention dimension (the \"rank\").\n",
      "    target_modules (`Optional[Union[List[str], str]]`):\n",
      "        The names of the modules to apply the adapter to. If this is specified, only the modules with the specified\n",
      "        names will be replaced. When passing a string, a regex match will be performed. When passing a list of\n",
      "        strings, either an exact match will be performed or it is checked if the name of the module ends with any\n",
      "        of the passed strings. If this is specified as 'all-linear', then all linear/Conv1D modules are chosen,\n",
      "        excluding the output layer. If this is not specified, modules will be chosen according to the model\n",
      "        architecture. If the architecture is not known, an error will be raised -- in this case, you should specify\n",
      "        the target modules manually.\n",
      "    lora_alpha (`int`):\n",
      "        The alpha parameter for Lora scaling.\n",
      "    lora_dropout (`float`):\n",
      "        The dropout probability for Lora layers.\n",
      "    fan_in_fan_out (`bool`):\n",
      "        Set this to True if the layer to replace stores weight like (fan_in, fan_out). For example, gpt-2 uses\n",
      "        `Conv1D` which stores weights like (fan_in, fan_out) and hence this should be set to `True`.\n",
      "    bias (`str`):\n",
      "        Bias type for LoRA. Can be 'none', 'all' or 'lora_only'. If 'all' or 'lora_only', the corresponding biases\n",
      "        will be updated during training. Be aware that this means that, even when disabling the adapters, the model\n",
      "        will not produce the same output as the base model would have without adaptation.\n",
      "    use_rslora (`bool`):\n",
      "        When set to True, uses <a href='https://doi.org/10.48550/arXiv.2312.03732'>Rank-Stabilized LoRA</a> which\n",
      "        sets the adapter scaling factor to `lora_alpha/math.sqrt(r)`, since it was proven to work better.\n",
      "        Otherwise, it will use the original default value of `lora_alpha/r`.\n",
      "    modules_to_save (`List[str]`):\n",
      "        List of modules apart from adapter layers to be set as trainable and saved in the final checkpoint.\n",
      "    init_lora_weights (`bool` | `Literal[\"gaussian\", \"loftq\"]`):\n",
      "        How to initialize the weights of the adapter layers. Passing True (default) results in the default\n",
      "        initialization from the reference implementation from Microsoft. Passing 'gaussian' results in Gaussian\n",
      "        initialization scaled by the LoRA rank for linear and layers. Setting the initialization to False leads to\n",
      "        completely random initialization and is discouraged. Pass `'loftq'` to use LoftQ initialization.\n",
      "    layers_to_transform (`Union[List[int], int]`):\n",
      "        The layer indices to transform. If a list of ints is passed, it will apply the adapter to the layer indices\n",
      "        that are specified in this list. If a single integer is passed, it will apply the transformations on the\n",
      "        layer at this index.\n",
      "    layers_pattern (`str`):\n",
      "        The layer pattern name, used only if `layers_to_transform` is different from `None`.\n",
      "    rank_pattern (`dict`):\n",
      "        The mapping from layer names or regexp expression to ranks which are different from the default rank\n",
      "        specified by `r`.\n",
      "    alpha_pattern (`dict`):\n",
      "        The mapping from layer names or regexp expression to alphas which are different from the default alpha\n",
      "        specified by `lora_alpha`.\n",
      "    megatron_config (`Optional[dict]`):\n",
      "        The TransformerConfig arguments for Megatron. It is used to create LoRA's parallel linear layer. You can\n",
      "        get it like this, `core_transformer_config_from_args(get_args())`, these two functions being from Megatron.\n",
      "        The arguments will be used to initialize the TransformerConfig of Megatron. You need to specify this\n",
      "        parameter when you want to apply LoRA to the ColumnParallelLinear and RowParallelLinear layers of megatron.\n",
      "    megatron_core (`Optional[str]`):\n",
      "        The core module from Megatron to use, defaults to `\"megatron.core\"`.\n",
      "    loftq_config (`Optional[LoftQConfig]`):\n",
      "        The configuration of LoftQ. If this is not None, then LoftQ will be used to quantize the backbone weights\n",
      "        and initialize Lora layers. Also pass `init_lora_weights='loftq'`. Note that you should not pass a\n",
      "        quantized model in this case, as LoftQ will quantize the model itself.\n",
      "    use_dora (`bool`):\n",
      "        Enable 'Weight-Decomposed Low-Rank Adaptation' (DoRA). This technique decomposes the updates of the weights\n",
      "        into two parts, magnitude and direction. Direction is handled by normal LoRA, whereas the magnitude is\n",
      "        handled by a separate learnable parameter. This can improve the performance of LoRA especially at low\n",
      "        ranks. Right now, DoRA only supports linear and Conv2D layers. DoRA introduces a bigger overhead than pure\n",
      "        LoRA, so it is recommended to merge weights for inference. For more information, see\n",
      "        https://arxiv.org/abs/2402.09353.\n",
      "    layer_replication(`List[Tuple[int, int]]`):\n",
      "        Build a new stack of layers by stacking the original model layers according to the ranges specified. This\n",
      "        allows expanding (or shrinking) the model without duplicating the base model weights. The new layers will\n",
      "        all have separate LoRA adapters attached to them.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/github/LLM-Workshop/personal_copilot/.copilot/lib/python3.11/site-packages/peft/tuners/lora/config.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     AdaLoraConfig"
     ]
    }
   ],
   "source": [
    "from peft.tuners.lora.config import LoraConfig\n",
    "LoraConfig?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Starcoder2ForCausalLM(\n",
       "      (model): Starcoder2Model(\n",
       "        (embed_tokens): Embedding(49152, 4608)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Starcoder2DecoderLayer(\n",
       "            (self_attn): Starcoder2FlashAttention2(\n",
       "              (q_proj): Linear4bit(in_features=4608, out_features=4608, bias=True)\n",
       "              (k_proj): Linear4bit(in_features=4608, out_features=512, bias=True)\n",
       "              (v_proj): Linear4bit(in_features=4608, out_features=512, bias=True)\n",
       "              (o_proj): Linear4bit(in_features=4608, out_features=4608, bias=True)\n",
       "              (rotary_emb): Starcoder2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Starcoder2MLP(\n",
       "              (c_fc): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4608, out_features=18432, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4608, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=18432, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=18432, out_features=4608, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=18432, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4608, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): LayerNorm((4608,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_layernorm): LayerNorm((4608,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((4608,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4608, out_features=49152, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mget_peft_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'PreTrainedModel'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpeft_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'PeftConfig'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmixed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'PeftModel | PeftMixedModel'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Returns a Peft model object from a model and a config.\n",
      "\n",
      "Args:\n",
      "    model ([`transformers.PreTrainedModel`]):\n",
      "        Model to be wrapped.\n",
      "    peft_config ([`PeftConfig`]):\n",
      "        Configuration object containing the parameters of the Peft model.\n",
      "    adapter_name (`str`, `optional`, defaults to `\"default\"`):\n",
      "        The name of the adapter to be injected, if not provided, the default adapter name is used (\"default\").\n",
      "    mixed (`bool`, `optional`, defaults to `False`):\n",
      "        Whether to allow mixing different (compatible) adapter types.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/github/LLM-Workshop/personal_copilot/.copilot/lib/python3.11/site-packages/peft/mapping.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "get_peft_model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure gradient checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient checkpointing is a technique used to reduce the memory usage when training deep learning models, at the cost of increased computation time. It's useful when training large models that would otherwise not fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    model.config.use_cache = not training_args.gradient_checkpointing\n",
    "```\n",
    "\n",
    "This line disables caching in the model configuration if gradient checkpointing is enabled. Caching is used to speed up computation by storing the results of expensive function calls and reusing them when the same inputs occur again. However, it increases memory usage, so it's disabled when gradient checkpointing is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_cache = not training_args.gradient_checkpointing\n",
    "model.config.use_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "    training_args.gradient_checkpointing = (\n",
    "        training_args.gradient_checkpointing and not model_args.use_unsloth\n",
    "    )\n",
    "    if training_args.gradient_checkpointing:\n",
    "        training_args.gradient_checkpointing_kwargs = {\n",
    "            \"use_reentrant\": model_args.use_reentrant\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enable gradient checkpointing only if it was initially enabled and `use_unsloth` is not set in the model arguments.\n",
    "\n",
    "If gradient checkpointing is enabled, we set the `use_reentrant` argument according to the provided input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.gradient_checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.gradient_checkpointing and not model_args.use_unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.use_reentrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.gradient_checkpointing = (\n",
    "    training_args.gradient_checkpointing and not model_args.use_unsloth\n",
    ")\n",
    "if training_args.gradient_checkpointing:\n",
    "    training_args.gradient_checkpointing_kwargs = {\n",
    "        \"use_reentrant\": model_args.use_reentrant\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.gradient_checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review all the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': 'bigcode/starcoder2-7b',\n",
       " 'lora_alpha': 64,\n",
       " 'lora_dropout': 0.0,\n",
       " 'lora_r': 32,\n",
       " 'lora_target_modules': 'c_proj,c_attn,q_attn,c_fc,c_proj',\n",
       " 'use_nested_quant': True,\n",
       " 'bnb_4bit_compute_dtype': 'bfloat16',\n",
       " 'bnb_4bit_quant_type': 'nf4',\n",
       " 'use_flash_attn': True,\n",
       " 'use_peft_lora': True,\n",
       " 'use_8bit_qunatization': False,\n",
       " 'use_4bit_quantization': True,\n",
       " 'use_reentrant': False,\n",
       " 'use_unsloth': False,\n",
       " 'use_loftq': False,\n",
       " 'use_loftq_callback': False}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_name': 'smangrul/hug_stack',\n",
       " 'dataset_text_field': 'text',\n",
       " 'max_seq_length': 1024,\n",
       " 'test_size': 0.1,\n",
       " 'fim_rate': 0.5,\n",
       " 'fim_spm_rate': 0.5,\n",
       " 'splits': 'train'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_dir': 'peft-lora-starcoder2-7b-personal-copilot-test',\n",
       " 'overwrite_output_dir': False,\n",
       " 'do_train': False,\n",
       " 'do_eval': True,\n",
       " 'do_predict': False,\n",
       " 'eval_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
       " 'prediction_loss_only': False,\n",
       " 'per_device_train_batch_size': 2,\n",
       " 'per_device_eval_batch_size': 2,\n",
       " 'per_gpu_train_batch_size': None,\n",
       " 'per_gpu_eval_batch_size': None,\n",
       " 'gradient_accumulation_steps': 4,\n",
       " 'eval_accumulation_steps': None,\n",
       " 'eval_delay': 0,\n",
       " 'learning_rate': 0.0005,\n",
       " 'weight_decay': 0.01,\n",
       " 'adam_beta1': 0.9,\n",
       " 'adam_beta2': 0.999,\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'max_grad_norm': 1.0,\n",
       " 'num_train_epochs': 3.0,\n",
       " 'max_steps': 1000,\n",
       " 'lr_scheduler_type': <SchedulerType.COSINE: 'cosine'>,\n",
       " 'lr_scheduler_kwargs': {},\n",
       " 'warmup_ratio': 0.0,\n",
       " 'warmup_steps': 30,\n",
       " 'log_level': 'passive',\n",
       " 'log_level_replica': 'warning',\n",
       " 'log_on_each_node': True,\n",
       " 'logging_dir': 'peft-lora-starcoder2-7b-personal-copilot-test/runs/May11_15-17-44_peace',\n",
       " 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
       " 'logging_first_step': False,\n",
       " 'logging_steps': 25,\n",
       " 'logging_nan_inf_filter': True,\n",
       " 'save_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
       " 'save_steps': 50,\n",
       " 'save_total_limit': None,\n",
       " 'save_safetensors': True,\n",
       " 'save_on_each_node': False,\n",
       " 'save_only_model': False,\n",
       " 'restore_callback_states_from_checkpoint': False,\n",
       " 'no_cuda': False,\n",
       " 'use_cpu': False,\n",
       " 'use_mps_device': False,\n",
       " 'seed': 42,\n",
       " 'data_seed': None,\n",
       " 'jit_mode_eval': False,\n",
       " 'use_ipex': False,\n",
       " 'bf16': True,\n",
       " 'fp16': False,\n",
       " 'fp16_opt_level': 'O1',\n",
       " 'half_precision_backend': 'auto',\n",
       " 'bf16_full_eval': False,\n",
       " 'fp16_full_eval': False,\n",
       " 'tf32': None,\n",
       " 'local_rank': 0,\n",
       " 'ddp_backend': None,\n",
       " 'tpu_num_cores': None,\n",
       " 'tpu_metrics_debug': False,\n",
       " 'debug': [],\n",
       " 'dataloader_drop_last': False,\n",
       " 'eval_steps': 50,\n",
       " 'dataloader_num_workers': 4,\n",
       " 'dataloader_prefetch_factor': None,\n",
       " 'past_index': -1,\n",
       " 'run_name': 'peft-lora-starcoder2-7b-personal-copilot-test',\n",
       " 'disable_tqdm': False,\n",
       " 'remove_unused_columns': True,\n",
       " 'label_names': None,\n",
       " 'load_best_model_at_end': False,\n",
       " 'metric_for_best_model': None,\n",
       " 'greater_is_better': None,\n",
       " 'ignore_data_skip': False,\n",
       " 'fsdp': [],\n",
       " 'fsdp_min_num_params': 0,\n",
       " 'fsdp_config': {'min_num_params': 0,\n",
       "  'xla': False,\n",
       "  'xla_fsdp_v2': False,\n",
       "  'xla_fsdp_grad_ckpt': False},\n",
       " 'fsdp_transformer_layer_cls_to_wrap': None,\n",
       " 'accelerator_config': AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None),\n",
       " 'deepspeed': None,\n",
       " 'label_smoothing_factor': 0.0,\n",
       " 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>,\n",
       " 'optim_args': None,\n",
       " 'adafactor': False,\n",
       " 'group_by_length': False,\n",
       " 'length_column_name': 'length',\n",
       " 'report_to': ['tensorboard', 'wandb'],\n",
       " 'ddp_find_unused_parameters': None,\n",
       " 'ddp_bucket_cap_mb': None,\n",
       " 'ddp_broadcast_buffers': None,\n",
       " 'dataloader_pin_memory': True,\n",
       " 'dataloader_persistent_workers': False,\n",
       " 'skip_memory_metrics': True,\n",
       " 'use_legacy_prediction_loop': False,\n",
       " 'push_to_hub': False,\n",
       " 'resume_from_checkpoint': None,\n",
       " 'hub_model_id': None,\n",
       " 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>,\n",
       " 'hub_token': None,\n",
       " 'hub_private_repo': False,\n",
       " 'hub_always_push': False,\n",
       " 'gradient_checkpointing': False,\n",
       " 'gradient_checkpointing_kwargs': None,\n",
       " 'include_inputs_for_metrics': False,\n",
       " 'eval_do_concat_batches': True,\n",
       " 'fp16_backend': 'auto',\n",
       " 'evaluation_strategy': None,\n",
       " 'push_to_hub_model_id': None,\n",
       " 'push_to_hub_organization': None,\n",
       " 'push_to_hub_token': None,\n",
       " 'mp_parameters': '',\n",
       " 'auto_find_batch_size': False,\n",
       " 'full_determinism': False,\n",
       " 'torchdynamo': None,\n",
       " 'ray_scope': 'last',\n",
       " 'ddp_timeout': 1800,\n",
       " 'torch_compile': False,\n",
       " 'torch_compile_backend': None,\n",
       " 'torch_compile_mode': None,\n",
       " 'dispatch_batches': None,\n",
       " 'split_batches': None,\n",
       " 'include_tokens_per_second': False,\n",
       " 'include_num_input_tokens_seen': False,\n",
       " 'neftune_noise_alpha': None,\n",
       " 'optim_target_modules': None,\n",
       " 'distributed_state': Distributed environment: DistributedType.NO\n",
       " Num processes: 1\n",
       " Process index: 0\n",
       " Local process index: 0\n",
       " Device: cuda,\n",
       " '_n_gpu': 2,\n",
       " '__cached__setup_devices': device(type='cuda', index=0),\n",
       " 'deepspeed_plugin': None}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss those parameters that we have not yet covered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size is recommended to be 2^N, often muliple of 8.\n",
    "\n",
    "[Tensor Core Requirements](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc) define the multiplier based on the dtype and the hardware. For instance, \n",
    "* for fp16 data type a multiple of 8 is recommended \n",
    "* but for an A100 GPU, a multiples of 64 is recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gradient accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient accumulation is a technique designed to compute gradients in smaller, more manageable increments rather than processing the entire batch simultaneously. This method involves a series of forward and backward passes through the model, during which gradients are calculated and accumulated. After a sufficient number of gradients have been gathered, the optimization step of the model is carried out. \n",
    "\n",
    "The advantage of using gradient accumulation is that it allows for an increase in the effective batch size, surpassing the constraints set by the GPU's memory. However, it's crucial to be aware that the extra forward and backward passes required by this method can potentially decelerate the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.per_device_train_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results in a 4x2 = 8 effective batch size on a single GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gradient checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient checkpointing is a technique that balances memory usage and computational speed during model training. Instead of storing all activations from the forward pass for gradient computation, which can consume significant memory, or discarding and recalculating them, which can slow down training, gradient checkpointing selectively saves certain activations. This means only a subset of activations need to be recalculated, optimizing both memory and computation resources.\n",
    "\n",
    "But it comes with a cost of [slowing down the training by approximately 20%](https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.gradient_checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixed precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed precision training is a method that enhances computational efficiency in model training by using lower-precision numerical formats for certain variables. While most models traditionally use 32-bit floating point precision (fp32), not all variables need this level of precision. By lowering the precision of some variables to formats like 16-bit floating point (fp16), computations can be sped up.\n",
    "\n",
    "Typically in mixed precision training: \n",
    "* Activations are in half precision (fp16)\n",
    "* Despite gradients being computed in half precision, they are converted back to full precision for optimization, so no memory is saved in this step. \n",
    "* It could also lead to more GPU memory being utilized, especially for small batch sizes. \n",
    "\n",
    "Newer GPU architectures, like the Ampere architecture, offer bf16 and tf32 data types. Tradditonal one is ft16.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(training_args.tf32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(training_args.bf16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainingArguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdata_collator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataCollator\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterableDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets.Dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0meval_dataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets.Dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreTrainedTokenizerBase\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_init\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvalPrediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainerCallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moptimizers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambdaLR\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpreprocess_logits_for_metrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers.\n",
      "\n",
      "Args:\n",
      "    model ([`PreTrainedModel`] or `torch.nn.Module`, *optional*):\n",
      "        The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.\n",
      "\n",
      "        <Tip>\n",
      "\n",
      "        [`Trainer`] is optimized to work with the [`PreTrainedModel`] provided by the library. You can still use\n",
      "        your own models defined as `torch.nn.Module` as long as they work the same way as the 🤗 Transformers\n",
      "        models.\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "    args ([`TrainingArguments`], *optional*):\n",
      "        The arguments to tweak for training. Will default to a basic instance of [`TrainingArguments`] with the\n",
      "        `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.\n",
      "    data_collator (`DataCollator`, *optional*):\n",
      "        The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will\n",
      "        default to [`default_data_collator`] if no `tokenizer` is provided, an instance of\n",
      "        [`DataCollatorWithPadding`] otherwise.\n",
      "    train_dataset (Union[`torch.utils.data.Dataset`, `torch.utils.data.IterableDataset`, `datasets.Dataset`], *optional*):\n",
      "        The dataset to use for training. If it is a [`~datasets.Dataset`], columns not accepted by the\n",
      "        `model.forward()` method are automatically removed.\n",
      "\n",
      "        Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a\n",
      "        distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a\n",
      "        `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will\n",
      "        manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally\n",
      "        sets the seed of the RNGs used.\n",
      "    eval_dataset (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`, `datasets.Dataset`]), *optional*):\n",
      "         The dataset to use for evaluation. If it is a [`~datasets.Dataset`], columns not accepted by the\n",
      "         `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each\n",
      "         dataset prepending the dictionary key to the metric name.\n",
      "    tokenizer ([`PreTrainedTokenizerBase`], *optional*):\n",
      "        The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs to the\n",
      "        maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an\n",
      "        interrupted training or reuse the fine-tuned model.\n",
      "    model_init (`Callable[[], PreTrainedModel]`, *optional*):\n",
      "        A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start\n",
      "        from a new instance of the model as given by this function.\n",
      "\n",
      "        The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to\n",
      "        be able to choose different architectures according to hyper parameters (such as layer count, sizes of\n",
      "        inner layers, dropout probabilities etc).\n",
      "    compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):\n",
      "        The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return\n",
      "        a dictionary string to metric values.\n",
      "    callbacks (List of [`TrainerCallback`], *optional*):\n",
      "        A list of callbacks to customize the training loop. Will add those to the list of default callbacks\n",
      "        detailed in [here](callback).\n",
      "\n",
      "        If you want to remove one of the default callbacks used, use the [`Trainer.remove_callback`] method.\n",
      "    optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*, defaults to `(None, None)`):\n",
      "        A tuple containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your\n",
      "        model and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.\n",
      "    preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*):\n",
      "        A function that preprocess the logits right before caching them at each evaluation step. Must take two\n",
      "        tensors, the logits and the labels, and return the logits once processed as desired. The modifications made\n",
      "        by this function will be reflected in the predictions received by `compute_metrics`.\n",
      "\n",
      "        Note that the labels (second parameter) will be `None` if the dataset does not have them.\n",
      "\n",
      "Important attributes:\n",
      "\n",
      "    - **model** -- Always points to the core model. If using a transformers model, it will be a [`PreTrainedModel`]\n",
      "      subclass.\n",
      "    - **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the\n",
      "      original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`,\n",
      "      the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner\n",
      "      model hasn't been wrapped, then `self.model_wrapped` is the same as `self.model`.\n",
      "    - **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from\n",
      "      data parallelism, this means some of the model layers are split on different GPUs).\n",
      "    - **place_model_on_device** -- Whether or not to automatically place the model on the device - it will be set\n",
      "      to `False` if model parallel or deepspeed is used, or if the default\n",
      "      `TrainingArguments.place_model_on_device` is overridden to return `False` .\n",
      "    - **is_in_train** -- Whether or not a model is currently running `train` (e.g. when `evaluate` is called while\n",
      "      in `train`)\n",
      "\u001b[0;31mFile:\u001b[0m           ~/github/LLM-Workshop/personal_copilot/.copilot/lib/python3.11/site-packages/transformers/trainer.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "Trainer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Starcoder2ForCausalLM(\n",
      "      (model): Starcoder2Model(\n",
      "        (embed_tokens): Embedding(49152, 4608)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x Starcoder2DecoderLayer(\n",
      "            (self_attn): Starcoder2FlashAttention2(\n",
      "              (q_proj): Linear4bit(in_features=4608, out_features=4608, bias=True)\n",
      "              (k_proj): Linear4bit(in_features=4608, out_features=512, bias=True)\n",
      "              (v_proj): Linear4bit(in_features=4608, out_features=512, bias=True)\n",
      "              (o_proj): Linear4bit(in_features=4608, out_features=4608, bias=True)\n",
      "              (rotary_emb): Starcoder2RotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Starcoder2MLP(\n",
      "              (c_fc): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4608, out_features=18432, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4608, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=18432, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=18432, out_features=4608, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=18432, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4608, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act): PytorchGELUTanh()\n",
      "            )\n",
      "            (input_layernorm): LayerNorm((4608,), eps=1e-05, elementwise_affine=True)\n",
      "            (post_attention_layernorm): LayerNorm((4608,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((4608,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4608, out_features=49152, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "trainer.accelerator.print(f\"{trainer.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.use_peft_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 47,185,920 || all params: 7,221,109,760 || trainable%: 0.6534\n"
     ]
    }
   ],
   "source": [
    "if model_args.use_peft_lora:\n",
    "    trainer.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loftq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For QLoRA training, when we're preparing to quantize the base model, it's worth considering the use of LoftQ initialization. This method has demonstrated its ability to enhance performance in conjunction with quantization. The underlying concept is to initialize the LoRA weights in a way that minimizes the quantization error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.use_loftq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoftQ initialization when using QLoRA\n",
    "if model_args.use_4bit_quantization and model_args.use_loftq:\n",
    "    loftq_init(trainer.model, tokenizer, train_dataset, data_args.max_seq_length ,model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mloftq_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Union[torch.Tensor, torch.nn.Parameter]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_bits\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreduced_rank\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[0;31mFile:\u001b[0m      ~/github/LLM-Workshop/personal_copilot/.copilot/lib/python3.11/site-packages/peft/utils/loftq_utils.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "from peft.utils.loftq_utils import loftq_init, replace_lora_weights_loftq\n",
    "loftq_init?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mreplace_lora_weights_loftq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpeft_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[str]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[Callable[[torch.nn.Module, str], bool]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Replace the LoRA weights of a model quantized with bitsandbytes, using the LoftQ technique.\n",
      "\n",
      "The replacement is done on the fly by loading in the non-quantized weights from a locally stored safetensors model\n",
      "file and initializing the LoRA weights such that the quantization error between the original and quantized weights\n",
      "is minimized.\n",
      "\n",
      "As lazy loading is not possible with pickle, normal PyTorch checkpoint files cannot be supported.\n",
      "\n",
      "Depending on the model size, calling this function may take some time to finish.\n",
      "\n",
      "Args:\n",
      "    peft_model (`PeftModel`):\n",
      "        The model to replace the weights of. Must be a quantized PEFT model with LoRA layers.\n",
      "    model_path (`Optional[str]`):\n",
      "        The path to the model safetensors file. If the model is a Hugging Face model, this will be inferred from\n",
      "        the model's config. Otherwise, it must be provided.\n",
      "    adapter_name (`str`):\n",
      "        The name of the adapter to replace the weights of. The default adapter name is \"default\".\n",
      "    callback (`Optional[Callable[[PeftModel, str], bool]]`):\n",
      "        A callback function that will be called after each module is replaced. The callback function should take\n",
      "        the model and the name of the current module as input and return a boolean indicating whether the\n",
      "        replacement should be kept. If the callback returns False, the replacement will be rolled back. This can be\n",
      "        very useful to confirm that the LoftQ initialization actually decreases the quantization error of the\n",
      "        model. As an example, this callback could generate logits for given input and compare it with the logits\n",
      "        from the original, non-quanitzed model with the same input, and only return `True` if there is an\n",
      "        improvement. As this is a greedy optimization, it's possible that calling this function multiple times\n",
      "        yields incremental improvements.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/github/LLM-Workshop/personal_copilot/.copilot/lib/python3.11/site-packages/peft/utils/loftq_utils.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "replace_lora_weights_loftq?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If enabled, `loftq_init` will call `replace_lora_weights_loftq` to replace the LoRA weights with LoftQ-initialized weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(training_args.resume_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = None\n",
    "if training_args.resume_from_checkpoint is not None:\n",
    "    checkpoint = training_args.resume_from_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train(resume_from_checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.is_fsdp_enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trainer.is_fsdp_enabled:\n",
    "    trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n",
    "\n",
    "#trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
